---
title: "Data acquired and computed - Version 2"
subtitle: "Quantities published and their origin"
author: "Pedro J. Aphalo (SenPEP)"
date: 2024-06-11
date-modified: 2024-06-11
date-meta: "`r Sys.Date()`"
abstract: |
  This page describes the data available from the weather station at the Viikki campus of the University of Helsinki, at Helsinki, Finland, since May 2024. 
keywords: [datalogger,data processing,published data]
categories: [Methods]
editor: 
  markdown: 
    wrap: 72
format:
  html: 
    code-fold: true
    code-link: true
    code-tools: true
draft: false
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(tidy = FALSE, cache = FALSE)
```

```{r, message=FALSE}
library(tidyverse)
library(ggridges)
library(ggpmisc)
library(lubridate)
```

# What makes data from this weather station different?

-   Time interval for acquisition of light measurements (From 2024-05-14):
    50 ms.

-   Time interval for logging of summaries (From 2024-05-14): 50 ms, 500 ms, 1 min, 1 h.

-   Solar radiation measurement: seven types of broadband
    sensors, to increase to eight.

-   Soil measurements as a depth profile.

-   Data summaries provided as a free service for research at the experimental
    field of the Viikki campus.

-   Data for state-of-the-art research on the spectrum of sunlight and
    its fast variation.

-   Open access to these data, deposited under a citable DOI.

# Data availability

[Open access through Open Science Foundation](https://osf.io/e4vau/)
under [DOI 10.17605/OSF.IO/E4VAU](https:doi.org/10.17605/OSF.IO/E4VAU).

To be cited, after adding date when the site was visited to download the
data, as:

Aphalo, Pedro J. 2024. _High Frequency Weather Data for Viikki,
Helsinki, Finland._ OSF. doi:10.17605/OSF.IO/E4VAU.

# Limitations of the data

Radiation data in winter months are parply unreliable as the sensors lack a
blower to keep the snow away. Some of the radiation sensors have heating but
this does not help much with sticky snow. Precipitation as snow or sleet is not
detected by the weather transmitter, and also wind measurements can be
potentially affected by snow and ice accumulation even with heating. Other
variables can be trusted year-round. The intention of this station was initially
to *take readings during the growing season*. The data in the files include
winter values for all variables, including those which are expected not reliable
on many days between mid November and late March, and a few days in April.

DATA ARE PROVIDED AS IS, WITH NO GUARANTEE, even if the station is managed
as carefully with the limited resources available.

# The data flow

The acquisition and summarizing of data requires multiples steps, some
performed on board the datalogger and some in a computer
[@fig-data-flow-chart]. The program in the logger is written in CRBasic,
data are downloaded through a USB connection to a laptop computer in the
field, using the program [PC400](https://www.campbellsci.com/pc400).
Later computations are done with R scripts that make use of packages
from the [R for Photobiology
Suite](https://www.r4photobiology.info/pages/r4p-introduction.html)
(Aphalo2025)

The scripts and logger program had to be rewritten to make it possible faster
measurements. Fewer computations are carried out on the logger and more are
done off-line. Data are logged in general at the fastest rate possible or needed
and value at slower rates are computed off-line. At the moment, only new data
are being processed with the new scripts. The operations are the same but
with different acquisition frequency slight differences are possible between
version 1 and version 2 data sets.

In version 2 a master script calls the other scripts. The intention is to
publish also daily summaries, which were not distributed until now.

```{mermaid}
%%| label: fig-data-flow-chart
%%| fig-cap: "Simplified flowchart of the steps from data acquisition to publication of the data."
flowchart TD
    A((Datalogger:\nData acquisition)) -.-> B
    B[Datalogger:\n Computation of summaries\n and data storage] -->|Download data| C
    C[Computer with PC400:\n Data download from logger] -->|Import data| D
    D[Computer with R:\n Read most recent raw data file\n and validate data] -.-> DD
    DD[Computer with R:\n Save data to .Rda file\n and validate data] -->|Read data| E
    E[Computer with R:\n Read all data from .Rda files] -.-> EE
    EE[Computer with R:\n Apply sensor calibrations based on dates] -.-> F
    F[Computer with R:\n Merge data into a single time series] -.-> G
    G[Computer with R:\n compute derived quantities] -.-> H
    H[Computer with R:\n prepare metadata and add it to R objects] -.-> I
    I[Computer with R:\n save to .Rda and .CSV files] -->|Upload| J((OSF repository:\n publish on line))
```

::: callout-note

Over the eight years during which the station has been operational,
there have been changes in the installed sensors (described under
[Equipment used and site description](station.equipment.qmd)), and in
the program run in the datalogger to acquire the data. Most of the
earlier changes to the program were related to the installation of new sensors.
However, recent changes to increase the data acquisition frequency meant that
the processing capacity and available memory in the logger became limiting.
To solve this problem we had to off-load some summary computations to
processing off-line.

This change does not affect the continuity of the published time series data, as
the two approaches are numerically equivalent.

The change in data acquisition frequency from once every 5 s to once
every 0.5 s on 2024-05-14, could in principle decrease the variability of
the 1 min summaries. However, the averaging period used for data
acquisition was decreased at the same time as the frequency was
increased. Means in the 1 min and 1 h time series are, thus, unlikely to
be affected significantly. Minima and maxima, and standard deviations
could, potentially be slightly affected.

Soil data are acquired once every 12 min, and 1 h means of 5 values logged. The
values returned by the Vaisala WXT520 weather transmitter are acquired once
every 10 s for the WXT536. The averaging period in the WXT536 is also 10 s, 1
min and 1 h averages should not be affected by this change. Minima and maxima,
and standard deviations could, potentially be slightly affected.

Data logging every 500 ms from data acquired every 50 ms is worthwhile only if
the variables being measured change this fast and the sensors have a suitably
short response time constant. The broad band light and UV sensors in use have a
time constant of 150 ms or better. The Apogee PAR sensor with analogue output
that we use has according to specifications a response time constant < 1 ms, so
data from this sensor are additionally logged during one 5-min burst per hour at
50 ms.

One consequence of these changes is the increased amount of data: for the 0.5 s
time step data raw-data files are approximately 1 GB per month. This makes
downloading data from the datalogger rather tedious.
:::

# Logged acquired and summary values

In this section I list the variables that are stored in the logger's
memory at different time intervals, plus the time and date-related
variables that are computed and adjusted to UTC when the data are
imported into R. The tables describe the variables as acquired and
logged starting on 2024-04-15.

## Data acquisition

Fast measurements from analogue sensors are given priority over the 
communication with sensors with digital serial interfaces using the
SDI12 protocol. The SDI12 protocol is used by the soil profile sensors and
the weather transmitter. Data from analogue sensors is acquired almost
synchronously, but not perfectly, as autoranging is used.

## Quantities logged once per half second

Logging of data once per 1 s tarted on 2024-04-15, thus the data listed
in @tbl-logged-second are not available before this date at this
frequency. They are nonetheless available as 1 min averages computed
from data acquired once every 5 s.

| Quantity          | Summary    | Units     |
|:------------------|:-----------|:----------|
| "TIMESTAMP"       | mean of 10 |           |
| "PAR_Den"         | mean of 10 | µmol/s/m² |
| "PAR_BF_tot"      | mean of 10 | µmol/s/m² |
| "PAR_BF_diff"     | mean of 10 | µmol/s/m² |
| "Red_Den_cal"     | mean of 10 | µmol/s/m² |
| "Far_red_Den_cal" | mean of 10 | µmol/s/m² |
| "Blue_Den"        | mean of 10 | µmol/s/m² |
| "UVA_Den"         | mean of 10 | µmol/s/m² |
| "UVB_Den"         | mean of 10 | µmol/s/m² |
| "SurfTemp_grnd"   | mean of 10 | C         |
| "SurfTemp_veg"    | mean of 10 | C         |

: Data logged once every 500 ms as means of 10 acquired values.
Column 'summary' shows how the data are summarised by the datalogger
before being stored, *sample* indicating that the most recently acquired
value is stored. The units are those obtained after off-line application
of calibration factors. {#tbl-logged-second}

## Quantities logged once per minute

The values currently logged are listed in @tbl-logged-minute. The
quantities logged once per minute are means of multiple data values
acquired by the data logger from the sensors. Until 2024-04-15 the data
acquired values acquired once every 5 s, from which the summaries listed
in @tbl-logged-minute are derived were not logged. Starting from
2024-04-15, data are being acquired more frequently, once every 1 s, and
thus the number of values used to compute 1 min means and other
summaries has increased from 12 to 60. In the case of the weather
variables acquired with the WXT weather transmitters, the data acquired
by the logger are summaries computed on-board of these "intelligent
sensors".

| Quantity              | Summary              | Units     |
|:----------------------|:---------------------|:----------|
| "TIMESTAMP"           | sample               |           |
| "WindSpd_S_WVT"       | mean of 6 values     | m/s       |
| "WindDir_D1_WVT"      | mean of 6 values     | Deg       |
| "WindDir_SD1_WVT"     | mean of 6 values     | Deg       |
| "AirTemp_Avg"         | mean of 6 values     | C         |
| "AirTemp_Min"         | min of 6 values      | C         |
| "AirTemp_Max"         | max of 6 values      | C         |
| "RelHumidity"         | mean of 6 values     | \%        |
| "AirDewPoint"         | mean of 6 values     | C         |
| "AirPressure"         | mean of 6 values     | hPa       |
| "Ramount_Tot"         | sum                  | mm        |
| "Hamount_Tot"         | sum                  | hits/cm2  |
| "T107_C_Avg(1-4)"     | mean of 12/60 values | C         |

: Data logged once per minute available before computation of derived
quantities. Column 'summary' shows how the data are summarised by the datalogger
before being stored. The units are those obtained after off-line application of
calibration factors. {#tbl-logged-minute}

## Quantities logged once per hour

The values currently logged are listed in @tbl-logged-hour,
respectively. These values are directly computed from the acquired data,
not from 1 min means.

| Quantity               | Summary                               | Units     |
|:-----------------------|:--------------------------------------|:----------|
| "TIMESTAMP"            | sample                                |           |
| "VWC\_\[1-3\]\_Avg"    | mean of 5 values                      | m3/m-3    |
| "EC\_\[1-3\]\_Avg"     | mean of 5 values                      | dS/m      |
| "T\_\[1-3\]\_Avg"      | mean of 5 values                      | C         |
| "VWC_5cm\_\[1-3\]"     | mean of 5 values                      | m3/m-3    |
| "Ka_5cm\_\[1-3\]"      | mean of 5 values                      | 1..80     |
| "T_5cm\_\[1-3\]"       | mean of 5 values                      | C         |
| "BulkEC_5cm\_\[1-3\]"  | mean of 5 values                      | dS/m      |
| "VWC_10cm\_\[1-3\]"    | mean of 5 values                      | m3/m-3    |
| "Ka_10cm\_\[1-3\]"     | mean of 5 values                      | 1..80     |
| "T_10cm\_\[1-3\]"      | mean of 5 values                      | C         |
| "BulkEC_10cm\_\[1-3\]" | mean of 5 values                      | dS/m      |
| "VWC_20cm\_\[1-3\]"    | mean of 5 values                      | m3/m-3    |
| "Ka_20cm\_\[1-3\]"     | mean of 5 values                      | 1..80     |
| "T_20cm\_\[1-3\]"      | mean of 5 values                      | C         |
| "BulkEC_20cm\_\[1-3\]" | mean of 5 values                      | dS/m      |
| "VWC_30cm\_\[1-3\]"    | mean of 5 values                      | m3/m-3    |
| "Ka_30cm\_\[1-3\]"     | mean of 5 values                      | 1..80     |
| "T_30cm\_\[1-3\]"      | mean of 5 values                      | C         |
| "BulkEC_30cm\_\[1-3\]" | mean of 5 values                      | dS/m      |
| "VWC_40cm\_\[1-3\]"    | mean of 5 values                      | m3/m-3    |
| "Ka_40cm\_\[1-3\]"     | mean of 5 values                      | 1..80     |
| "T_40cm\_\[1-3\]"      | mean of 5 values                      | C         |
| "BulkEC_40cm\_\[1-3\]" | mean of 5 values                      | dS/m      |
| "VWC_50cm\_\[1-3\]"    | mean of 5 values                      | m3/m-3    |
| "Ka_50cm\_\[1-3\]"     | mean of 5 values                      | 1..80     |
| "T_50cm\_\[1-3\]"      | mean of 5 values                      | C         |
| "BulkEC_50cm\_\[1-3\]" | mean of 5 values                      | dS/m      |

: Data logged once per hour available before computation of derived
quantities. Column 'summary' shows how the data are summarised by the
datalogger before being stored. When the number of values is indicated
as 720/3600, 720 is for data acquired before 2024-04-15 and 3600 for
more recent data. The units are those obtained after off-line
application of calibration factors. As for data logged at 1 min
intervals, the TIMESTAMP is converted to UTC time. Sun angles are not
computed. {#tbl-logged-hour}

# Data processing in R

The logged data are imported into R from files created by Campbell
Scientific's PC400 program. The files are read using function
read_csi_dat()\` from package 'photobiologyInOut'. The data now in data
frames are checked for duplicate rows, outlier values, and time shifts.
Problems are corrected when possible or data replaced by NAs when
recovery is not possible.

In a separate step, R scripts are used to merge data in the different files
created during import into R. Validated calibrations and corrections are applied
obtaining a single data object with all data logged at a given frequency and new
summaries computed for variables logged at higher frequencies. The new scripts
do additional computations than those from version 1 including combining the
data from the three soil profile sensors into single median values. This is most
noticeable for data at 1 h and 1 d time steps. In the case of daily data we
use days at UTC rather than local time. As far as I am aware this is the same
approach used by the Finnish Meteorological Institute. The time for hourly
summaries follows UTC and matches approach used by the Finnish Meteorological 
Institute, the reported time is that at the start/end of the hour for which
data are averaged.

Additional quantities are calculated and added to the data set, including solar
time, sun elevation and sun azimuth based on geographic coordinates and UTC time
[@tbl-r-script]. The position of the sun and solar time are computed using Meuss
(1998) very accurate equations as implemented in R package 'photobiology'. The
coordinates of the weather station are for the locations of the mast, with
radiation sensors within 1.5 m of it, and soil sensors within 10 m. The accuracy
is that allowed by the image layer of the free version of Google Maps with high
resolution imagery.

| Quantity           | Summary   | Units                     |
|:-------------------|:----------|:--------------------------|
| "time"             | sample    | yyyy-mm-dd hh:mm:ss UTC   |
| "day_of_year"      | sample    | numeric                   |
| "month_of_year"    | sample    | 1..12                     |
| "month_name"       | sample    | character                 |
| "calendar_year"    | sample    | numeric                   |
| "time_of_day"      | sample    | numeric                   |
| "solar_time"       | sample    | numeric                   |
| "sun_elevation"    | sample    | degrees                   |
| "sun_azimuth"      | sample    | degrees                   |
| "PAR_BF_diff_fr"   | sample    | /1                        |
| "PAR_diff_fr_rel"  | sample    | /1                        |
| "was_sunny"        | sample    | logical                   |

: Some quantities are computed and added to the data set. Based on the
"TIMESTAMP" stored by the datalogger (and its difference from UTC time)
and the geographic coordinates of the station, the sun position and
solar time are computed and added to the data. "TIMESTAMP" is deleted
and replaced by "time" and the quantities in this table. {#tbl-r-script}

Most calibrations are applied at this stage. (To facilitate monitoring a
calibration is used in the logger for most sensors, but small additional
adjustments based on calibrations done after the logger program was written are
applied off-line)

Variable `was_sunny` indicates occlusion or not of the solar disk,
estimated based on the deviation from the expected fraction of diffuse
PAR at the current sun elevation. The reference value is derived from
spectral irradiance simulations done with the TUV radiation transfer and
atmosphere photochemistry model TUV. The variable is set to `NA` when
the sun is below the horizon or diffuse PAR irradiance is less than 5
$\mathrm{\mu mol\,m^{-2}\,s^{-1}}$.

The irradiances of UV-A1 and UV-A2 are derived from the measured UV-A,
UV-B and blue irradiances, based on calibrations on-site against a
spectrometer. The UV-A irradiance measurements are also corrected for
the temperature coefficient of the sensor as published [Langer
2020](https://download.sglux.de/publications/sglux_SiC_Temperature_Coefficient.pdf),
taking into account spectral dependence of the temperature and radiation
responses and the shape of the solar spectrum.

A new UV-A1 sensor will put in service during 2024.

# Information about the published data

In the examples below, we use R to read the data from `.rda` files, but
the same data are also available as comma separated values (CSV) in Gnu
Zip (gzip) compressed files.

```{r}
load("F:/aphalo/research-data/data-weather/RFR-Viikki-field/data-rda/second_2024_latest.tb.rda")
cat(comment(second_2024_latest.tb))
```

**The data set at 1 second interval** has
`r nrow(second_2024_latest.tb)` rows and `r ncol(second_2024_latest.tb)`
variables.

The variables are
`r paste(colnames(second_2024_latest.tb), collapse = ", ")`.

```{r}
load("F:/aphalo/research-data/data-weather/RFR-Viikki-field/data-rda/minute_2024_latest.tb.rda")
cat(comment(minute_2024_latest.tb))
```

**The data set at 1 min interval** has `r nrow(minute_2024_latest.tb)`
rows and `r ncol(minute_2024_latest.tb)` variables.

The variables are
`r paste(colnames(minute_2024_latest.tb), collapse = ", ")`.

```{r}
load("F:/aphalo/research-data/data-weather/RFR-Viikki-field/data-rda/hour_2024_latest.tb.rda")
cat(comment(hour_2024_latest.tb))
```

**The data set at 1 h interval** has
`r nrow(hour_2024_latest.tb)` rows and
`r ncol(hour_2024_latest.tb)` variables. The variables are
`r paste(colnames(hour_2024_latest.tb), collapse = ", ")`.

```{r}
load("F:/aphalo/research-data/data-weather/RFR-Viikki-field/data-rda/day_2024_latest.tb.rda")
cat(comment(hour_2024_latest.tb))
```

**The data set at 1 d interval** has
`r nrow(day_2024_latest.tb)` rows and
`r ncol(day_2024_latest.tb)` variables. The variables are
`r paste(colnames(day_2024_latest.tb), collapse = ", ")`.

::: callout-note
A detailed data dictionary will be added soon to the data objects
themselves. Data are in SI units and in most cases without scale
factors. The units described for the logged data in the tables above are
not modified except for sensors for which mV signals are logged.
:::

# Example plots

Using data saved at 1 min interval we can compute empirical desity
distributions, showing how frequently different values have been
observed.

## PAR photon irradiance

Photosynthetically active radiation photon irradiance at Viikki,
Helsinki, presented as fitted empirical density functions for each month
of the year [@fig-PAR-month-sec, @fig-PAR-month-min]. During winter-time some snow from time to
time accumulated on the sensor may have distorted the distribution to
some extent. (Each curve in the plot can be thought as equivalent to a
smoothed-out histogram, scaled to an area of one.)

```{r}
#| label: fig-PAR-month-sec
#| fig-cap: Empirical density of photosynthetically active radiation (PAR) irradiances by month of the year. The density curves have been fitted to 0.5 s averages computed from data acquired once every 50 ms.
second_2024_latest.tb %>%
  filter(sun_elevation > 0 & PAR_umol > 0) %>%
  select(month_name, PAR_umol, calendar_year) %>%
  na.omit() %>%
  ggplot(aes(x = PAR_umol, y = month_name)) +
  stat_panel_counts(label.y = "top", label.x = "right", size = 3.5) +
  geom_density_ridges(na.rm = TRUE, scale = 2.5, fill = "yellow", alpha = 0.33) +
  scale_x_log10(breaks = c(0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000), 
                minor_breaks = NULL,
                labels = scales::format_format(digits = 3, scientific = FALSE)) +
  scale_y_discrete(drop=FALSE, expand = expansion(mult = c(0.05, 0.2))) +
  coord_cartesian(xlim = c(0.5, 3000)) +
  labs(x = expression(PAR~~photon~~irradiance~~(mu*mol~m^{-2}~s^{-1})), y = "Month of the year") -> par_month_rigdes.fig
par_month_rigdes.fig
```

```{r}
#| label: fig-PAR-month-min
#| fig-cap: Empirical density of photosynthetically active radiation (PAR) irradiances by month of the year. The density curves have been fitted to 1 min averages computed from data acquired once every 50 ms.
minute_2024_latest.tb %>%
  filter(sun_elevation > 0 & PAR_umol > 0) %>%
  select(month_name, PAR_umol, calendar_year) %>%
  na.omit() %>%
  ggplot(aes(x = PAR_umol, y = month_name)) +
  stat_panel_counts(label.y = "top", label.x = "right", size = 3.5) +
  geom_density_ridges(na.rm = TRUE, scale = 2.5, fill = "yellow", alpha = 0.33) +
  scale_x_log10(breaks = c(0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000), 
                minor_breaks = NULL,
                labels = scales::format_format(digits = 3, scientific = FALSE)) +
  scale_y_discrete(drop=FALSE, expand = expansion(mult = c(0.05, 0.2))) +
  coord_cartesian(xlim = c(0.5, 3000)) +
  labs(x = expression(PAR~~photon~~irradiance~~(mu*mol~m^{-2}~s^{-1})), y = "Month of the year") -> par_month_rigdes.fig
par_month_rigdes.fig
```


```{r}
#| label: fig-PAR-month-hour
#| fig-cap: Empirical density of photosynthetically active radiation (PAR) irradiances by month of the year. The density curves have been fitted to 1 h averages computed from data acquired once every 50 ms.
hour_2024_latest.tb %>%
  filter(sun_elevation > 0 & PAR_umol_mean > 0) %>%
  select(month_name, PAR_umol_mean, calendar_year) %>%
  na.omit() %>%
  ggplot(aes(x = PAR_umol_mean, y = month_name)) +
  stat_panel_counts(label.y = "top", label.x = "right", size = 3.5) +
  geom_density_ridges(na.rm = TRUE, scale = 2.5, fill = "yellow", alpha = 0.33) +
  scale_x_log10(breaks = c(0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000), 
                minor_breaks = NULL,
                labels = scales::format_format(digits = 3, scientific = FALSE)) +
  scale_y_discrete(drop=FALSE, expand = expansion(mult = c(0.05, 0.2))) +
  coord_cartesian(xlim = c(0.5, 3000)) +
  labs(x = expression(PAR~~photon~~irradiance~~(mu*mol~m^{-2}~s^{-1})), y = "Month of the year") -> par_month_rigdes.fig
par_month_rigdes.fig
```

## Soil temperature profile

```{r}
load("F:/aphalo/research-data/data-weather/RFR-Viikki-field/data-rda/hour_2024_latest.tb.rda")
```

```{r}
hour_2024_latest.tb %>%
  select(time, month_of_year, day_of_year, calendar_year, starts_with("T_") & ends_with("cm")) %>%
  pivot_longer(starts_with("T_"), names_to = "layer", values_to = "soil_t") %>%
  mutate(depth = as.numeric(gsub("T_|cm", "", layer))) -> soil_temperature.tb
```

In @fig-soil-temp-depth the soil temperature depth profile is shown. It
is summarised using quantile regression for the 5%, 50% and 95%
percentiles. This highlights the range of variation at different depths
on different months of the year. The data values, not plotted, are
medians from three sensors, logged once every 1 h..

```{r, warning=FALSE, message=FALSE}
#| label: fig-soil-temp-depth
#| fig-cap: Soil-temperature depth profile for each month of the year.
soil_temperature.tb %>%
  na.omit() %>%
  ggplot(aes(depth, soil_t)) +
  stat_quant_band(formula = y ~ poly(x, 5, raw = TRUE), quantiles = c(0.05, 0.5, 0.95)) +
  labs(y = "Soil temperature (C)", x = "Depth (cm)") +
  facet_wrap(~factor(month.name[month_of_year], levels = month.name[unique(month_of_year)])) +
  xlim(50, 0) +
#  scale_x_continuous(trans = "reverse") +
  coord_flip() -> soil_t.fig
soil_t.fig
```

In @fig-soil-temp-time-course we show hourly temperatures at different
depths over a few days, showing the daily warming and cooling of the
mostly bare soil. The data values shown are medians from three sensors,
acquired once every 1 h.

```{r, warning=FALSE, message=FALSE}
#| label: fig-soil-temp-time-course
#| fig-cap: Soil-temperature time course at nine different depths during nine consecutive days in the Summer of year 2020. Data logged once per every 1 h.

soil_temperature.tb %>%
#  filter(day_of_year > 190 & day_of_year < 200 & calendar_year == 2024) %>%
  ggplot(aes(time, soil_t, colour = factor(depth))) +
  geom_line() +
  labs(y = "Soil temperature (C)", x = "Time",
       colour = "Depth (cm)")  -> soil_t_daily.fig
soil_t_daily.fig
```
